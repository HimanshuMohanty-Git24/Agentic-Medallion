{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "302fba4b",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# Medallion Architecture Data Pipeline with LangChain Agents\n",
    "Building a Modern Data Lakehouse using GenAI-powered ETL Orchestration\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e106c5d9",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## Cell 1: Install Dependencies and Restart Python\n",
    "This cell ensures all necessary Python libraries for the data pipeline are installed. langchain, langchain-community, langchain-core, databricks-langchain, langgraph, playwright, and Pillow are required for building and running the agentic pipeline. dbutils.library.restartPython() is crucial in Databricks notebooks to ensure newly installed libraries are immediately available.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b43fb25",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install langchain langchain-community langchain-core databricks-langchain langgraph playwright Pillow\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deca6f5",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## Cell 2: Configure LangChain and LangSmith\n",
    "This cell sets up environment variables for LangChain and LangSmith. LangSmith is used for tracing, monitoring, and debugging LangChain applications. It's configured with an endpoint, API key (which you should replace with your actual key), and a project name for organizing runs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d0b3ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR_LANGSMITH_API_KEY\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Databricks - Medallion Pipeline\"\n",
    "\n",
    "print(f\"LangSmith configured. Project: '{os.environ['LANGCHAIN_PROJECT']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbed674e",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## Cell 3: Import Dependencies\n",
    "Setting up the foundation with comprehensive library imports to enable:\n",
    "- LangChain integration with Databricks for LLM-powered agents\n",
    "- LangGraph components for orchestrating the agent workflow\n",
    "- PySpark SQL capabilities for scalable data transformations\n",
    "- Core data science libraries for analysis and visualization\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0241ec9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "from typing import Annotated, TypedDict, List, Optional\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, when, lit, md5, concat_ws, expr, to_date, upper, sum as _sum, avg as _avg, count as _count, date_format, year, month\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e876d",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## Cell 4: Initialize Spark Session, LLM, and Database Schemas\n",
    "\n",
    "This cell initializes the SparkSession, the large language model (LLM) for agentic operations, and creates the necessary database schemas (Bronze, Silver, Gold) following the Medallion Architecture pattern. A log_event utility function is also defined here to provide structured, colored logging throughout the pipeline execution.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f256bba8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def log_event(level, source, message):\n",
    "    colors = {\"AGENT\": \"\\033[94m\", \"TOOL\": \"\\033[93m\", \"INFO\": \"\\033[92m\", \"ERROR\": \"\\033[91m\", \"GRAPH\": \"\\033[95m\"}\n",
    "    reset_color = \"\\033[0m\"\n",
    "    timestamp = datetime.now().strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "    print(f\"{timestamp} | {colors.get(level, '')}{level:<7}{reset_color} | {colors.get(level, '')}[{source}]{reset_color} {message}\")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AgenticDataPipeline\").getOrCreate()\n",
    "llm = ChatDatabricks(endpoint=\"databricks-claude-3-7-sonnet\", temperature=0.0, max_tokens=8000)\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS ops_bronze\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS ops_silver\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS ops_gold\")\n",
    "\n",
    "log_event(\"INFO\", \"Setup\", \"Environment initialized with Claude 3.7 Sonnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238b1a3",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## Cell 5: Generate Bronze Layer Raw Data\n",
    "\n",
    "This cell creates synthetic raw data for the Bronze layer. It generates data for customers_raw, transactions_raw, accounts_raw, and opportunities_raw tables within the ops_bronze schema. This step simulates the ingestion of raw, potentially messy, source data into the data lake.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43f8194",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_bronze_data():\n",
    "    log_event(\"INFO\", \"DataGen\", \"Generating Bronze layer data...\")\n",
    "    \n",
    "    customer_data = []\n",
    "    for i in range(1, 101):\n",
    "        name = f\"FName{i} LName{i}\" if random.random() > 0.1 else None\n",
    "        email = f\"fname{i}.lname{i}@email.com\" if random.random() > 0.1 else \"invalid-email\"\n",
    "        address = f\"{i*10} Main St\" if random.random() > 0.05 else None\n",
    "        customer_data.append((i, name, email, address, f\"2023-01-{random.randint(10, 28)}\"))\n",
    "    \n",
    "    customer_data.extend([\n",
    "        (1, 'John Doe', 'john.doe@email.com', '123 Elm St', '2023-01-15'), \n",
    "        (3, 'Peter Jones', 'peter.jones.dup@email.com', '789 Pine Ln', '2023-01-17')\n",
    "    ])\n",
    "    spark.createDataFrame(customer_data, [\"customer_id\", \"name\", \"email\", \"address\", \"join_date\"]).write.mode(\"overwrite\").saveAsTable(\"ops_bronze.customers_raw\")\n",
    "\n",
    "    transactions_data = []\n",
    "    for i in range(1, 201):\n",
    "        cust_id = random.randint(1, 110)\n",
    "        qty = random.randint(-5, 50)\n",
    "        amount = f\"${random.uniform(5, 1000):.2f}\" if random.random() > 0.05 else None\n",
    "        transactions_data.append((100+i, cust_id, qty, amount, f\"2023-02-{random.randint(1, 28)}\"))\n",
    "    spark.createDataFrame(transactions_data, [\"transaction_id\", \"customer_id\", \"quantity\", \"amount\", \"transaction_date\"]).write.mode(\"overwrite\").saveAsTable(\"ops_bronze.transactions_raw\")\n",
    "\n",
    "    accounts_data = [('ACC{:03d}'.format(i), f'GlobalCorp-{i}', random.choice(['Technology', 'Healthcare', 'Finance', 'TECH', None]), random.choice(['USA', 'UK', 'Germany'])) for i in range(1, 51)]\n",
    "    accounts_data.append(('ACC001', 'Global Corporation', 'Tech', 'USA'))\n",
    "    spark.createDataFrame(accounts_data, [\"account_id\", \"account_name\", \"industry\", \"region\"]).write.mode(\"overwrite\").saveAsTable(\"ops_bronze.accounts_raw\")\n",
    "    \n",
    "    opportunities_data = [(f'OPP{i:03d}', f\"ACC{random.randint(1, 55):03d}\", random.randint(-1000, 200000), '2024-07-15', random.choice(['Closed Won', 'Negotiation', 'Proposal', 'Qualification', 'Closed Lost'])) for i in range(1, 151)]\n",
    "    spark.createDataFrame(opportunities_data, [\"opportunity_id\", \"account_id\", \"value\", \"close_date\", \"stage\"]).write.mode(\"overwrite\").saveAsTable(\"ops_bronze.opportunities_raw\")\n",
    "    \n",
    "    log_event(\"INFO\", \"DataGen\", \"Bronze data creation complete\")\n",
    "    display(spark.sql(\"SHOW TABLES IN ops_bronze\"))\n",
    "\n",
    "create_bronze_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1925d440",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## Cell 6: Define Agent Tools\n",
    "\n",
    "This cell defines the tool functions that the agents within the LangGraph workflow can use:\n",
    "\n",
    "- **get_table_info**: Profiles a given table, providing schema, row count, and a data preview.\n",
    "- **execute_pyspark_code**: Safely executes generated PySpark code within the Spark environment, handling errors. \n",
    "- **create_notebook_visualization**: Generates and displays basic visualizations directly within the Databricks notebook.\n",
    "\n",
    "These tools empower the LLM agents to interact with the data and environment.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52b67d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_table_info(table_name: str) -> str:\n",
    "    \"\"\"Provides comprehensive table profiling including schema, statistics, and data preview for transformation planning.\"\"\"\n",
    "    try:\n",
    "        log_event(\"TOOL\", \"get_table_info\", f\"Profiling: {table_name}\")\n",
    "        df = spark.table(table_name)\n",
    "        schema_str = \"\\n\".join([f\"- {field.name}: {str(field.dataType)}\" for field in df.schema.fields])\n",
    "        count = df.count()\n",
    "        preview = df.limit(3).toPandas().to_string()\n",
    "        return f\"TABLE: {table_name}\\nROW_COUNT: {count}\\n\\nSCHEMA:\\n{schema_str}\\n\\nSAMPLE_DATA:\\n{preview}\"\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def execute_pyspark_code(code: str) -> str:\n",
    "    \"\"\"Executes PySpark transformation code with comprehensive error handling and validation.\"\"\"\n",
    "    try:\n",
    "        log_event(\"TOOL\", \"execute_pyspark_code\", f\"Executing transformation...\")\n",
    "        exec_globals = {\n",
    "            'spark': spark, 'col': col, 'regexp_replace': regexp_replace, 'when': when, 'lit': lit,\n",
    "            'md5': md5, 'concat_ws': concat_ws, 'expr': expr, 'to_date': to_date, 'upper': upper,\n",
    "            '_sum': _sum, '_avg': _avg, '_count': _count, 'date_format': date_format, 'year': year, 'month': month, 'Window': Window\n",
    "        }\n",
    "        exec(code, exec_globals)\n",
    "        log_event(\"TOOL\", \"execute_pyspark_code\", \"SUCCESS\")\n",
    "        return json.dumps({\"status\": \"success\", \"error\": None})\n",
    "    except Exception as e:\n",
    "        error_msg = f\"EXECUTION_ERROR: {str(e)}\"\n",
    "        log_event(\"ERROR\", \"execute_pyspark_code\", error_msg)\n",
    "        return json.dumps({\"status\": \"error\", \"error\": error_msg})\n",
    "        \n",
    "@tool\n",
    "def create_notebook_visualization(table_name: str, plot_type: str, x_col: str, y_col: str, title: str) -> str:\n",
    "    \"\"\"Creates optimized dashboard visualizations with automatic data sorting and limiting for performance.\"\"\"\n",
    "    try:\n",
    "        log_event(\"TOOL\", \"create_visualization\", f\"Creating: {title}\")\n",
    "        df = spark.table(table_name)\n",
    "        if plot_type == 'bar' and 'Top' in title:\n",
    "            df_display = df.orderBy(col(y_col).desc()).limit(5)\n",
    "        else:\n",
    "            df_display = df.orderBy(x_col)\n",
    "        print(f\"\\n=== DASHBOARD COMPONENT: {title} ===\")\n",
    "        display(df_display)\n",
    "        return f\"SUCCESS: '{title}' visualization created and displayed\"\n",
    "    except Exception as e:\n",
    "        return f\"VISUALIZATION_ERROR: {str(e)}\"\n",
    "\n",
    "all_tools = [get_table_info, execute_pyspark_code, create_notebook_visualization]\n",
    "tool_node = ToolNode(all_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a44a1d",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## Cell 7: Define Agent States, Nodes, and Graph Workflow\n",
    "\n",
    "This is the core of the agentic pipeline.\n",
    "\n",
    "### AgentState\n",
    "Defines the shared state object passed between agents in the LangGraph workflow, including messages, current task, PySpark code, review feedback, execution errors, and retry count.\n",
    "\n",
    "### Agent Functions\n",
    "\n",
    "- **planner_agent**: Generates a detailed data transformation plan\n",
    "- **code_generator_agent**: Writes PySpark code based on the plan and feedback\n",
    "- **code_reviewer_agent**: Reviews the generated code for quality and correctness \n",
    "- **prepare_for_execution**: Formats the PySpark code for tool execution\n",
    "\n",
    "### Conditional Edges\n",
    "- **after_review_decider**: Routes based on code review feedback\n",
    "- **after_execution_decider**: Routes based on execution results\n",
    "These enable an autonomous self-correction loop.\n",
    "\n",
    "### Workflow Definition \n",
    "StateGraph defines the nodes (agents and tools) and edges (transitions) of the Medallion pipeline workflow.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc59496",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, lambda x, y: x + y]\n",
    "    current_task: str\n",
    "    pyspark_code: Optional[str]\n",
    "    review_feedback: Optional[str]\n",
    "    execution_error: Optional[str]\n",
    "    retry_count: int\n",
    "\n",
    "def planner_agent(state: AgentState):\n",
    "    log_event(\"AGENT\", \"Planner\", \"Creating transformation strategy\")\n",
    "    \n",
    "    system_prompt = \"\"\"You are an expert data architect specializing in Databricks Medallion architecture. \n",
    "    \n",
    "    OBJECTIVES:\n",
    "    - Create precise, executable PySpark transformation plans\n",
    "    - Ensure data quality, deduplication, and proper type casting\n",
    "    - Follow medallion best practices (Bronze=raw, Silver=cleaned, Gold=aggregated)\n",
    "    - Design for scalability and performance optimization\n",
    "    \n",
    "    OUTPUT FORMAT:\n",
    "    Provide a detailed, step-by-step technical plan with:\n",
    "    1. Data profiling requirements\n",
    "    2. Specific transformation logic\n",
    "    3. Quality checks and validation rules\n",
    "    4. Performance optimization strategies\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"TRANSFORMATION TASK: {state['current_task']}\n",
    "    \n",
    "    Create a comprehensive technical plan that addresses:\n",
    "    - Data quality issues and remediation\n",
    "    - Schema standardization and type casting\n",
    "    - Deduplication strategies  \n",
    "    - Business rule implementation\n",
    "    - Performance optimization techniques\"\"\"\n",
    "    \n",
    "    messages = [HumanMessage(content=f\"{system_prompt}\\n\\n{user_prompt}\")]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [AIMessage(content=response.content, name=\"PlannerAgent\")]}\n",
    "\n",
    "def code_generator_agent(state: AgentState):\n",
    "    log_event(\"AGENT\", \"CodeGenerator\", \"Generating PySpark implementation\")\n",
    "    \n",
    "    plan = next((msg.content for msg in state['messages'] if isinstance(msg, AIMessage) and msg.name == \"PlannerAgent\"), \"\")\n",
    "    \n",
    "    context_prompt = \"\"\n",
    "    if state.get('review_feedback') and \"APPROVED\" not in state['review_feedback'].upper():\n",
    "        context_prompt = f\"\\n\\nREVIEW FEEDBACK TO ADDRESS:\\n{state['review_feedback']}\"\n",
    "    elif state.get('execution_error'):\n",
    "        context_prompt = f\"\\n\\nEXECUTION ERROR TO FIX:\\n{state['execution_error']}\"\n",
    "\n",
    "    system_prompt = \"\"\"You are a senior PySpark developer with expertise in Databricks optimization.\n",
    "\n",
    "    CODING REQUIREMENTS:\n",
    "    - Generate complete, executable PySpark code only\n",
    "    - Use explicit imports for all functions\n",
    "    - Implement robust error handling and data validation\n",
    "    - Apply performance optimizations (caching, partitioning)\n",
    "    - Follow PySpark best practices for large-scale data processing\n",
    "    - Use .mode(\"overwrite\").saveAsTable() for all writes\n",
    "    \n",
    "    CRITICAL CONSTRAINTS:\n",
    "    - NO explanatory text, comments, or markdown\n",
    "    - SparkSession 'spark' is pre-initialized\n",
    "    - Code must be production-ready and optimized\n",
    "    - Handle edge cases and null values appropriately\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"IMPLEMENTATION PLAN:\\n{plan}{context_prompt}\n",
    "    \n",
    "    Generate the complete PySpark transformation code that:\n",
    "    1. Implements all requirements from the plan\n",
    "    2. Handles data quality issues robustly\n",
    "    3. Optimizes for performance and scalability\n",
    "    4. Produces clean, reliable output tables\"\"\"\n",
    "\n",
    "    messages = [HumanMessage(content=f\"{system_prompt}\\n\\n{user_prompt}\")]\n",
    "    response = llm.invoke(messages)\n",
    "    clean_code = response.content.strip().replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
    "    return {\"pyspark_code\": clean_code, \"execution_error\": None, \"review_feedback\": None}\n",
    "\n",
    "def code_reviewer_agent(state: AgentState):\n",
    "    log_event(\"AGENT\", \"CodeReviewer\", \"Performing quality assurance review\")\n",
    "    \n",
    "    plan = next((msg.content for msg in state['messages'] if isinstance(msg, AIMessage) and msg.name == \"PlannerAgent\"), \"\")\n",
    "    \n",
    "    system_prompt = \"\"\"You are a senior data engineering QA specialist with deep PySpark expertise.\n",
    "\n",
    "    REVIEW CRITERIA:\n",
    "    - Code completeness and correctness\n",
    "    - Performance optimization implementation\n",
    "    - Data quality and validation logic\n",
    "    - Error handling robustness\n",
    "    - Adherence to transformation requirements\n",
    "    - Production readiness standards\n",
    "\n",
    "    RESPONSE FORMAT:\n",
    "    - If code meets ALL requirements: respond with exactly \"APPROVED\"\n",
    "    - If issues exist: respond with \"REJECTION_REASON: [specific actionable feedback]\"\n",
    "    \n",
    "    Be thorough but decisive. Code must be production-quality.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"ORIGINAL PLAN:\\n{plan}\\n\\nIMPLEMENTATION CODE:\\n```python\\n{state['pyspark_code']}\\n```\n",
    "    \n",
    "    Evaluate if the code fully implements the plan requirements with production-quality standards.\"\"\"\n",
    "\n",
    "    messages = [HumanMessage(content=f\"{system_prompt}\\n\\n{user_prompt}\")]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"review_feedback\": response.content}\n",
    "\n",
    "def prepare_for_execution(state: AgentState):\n",
    "    log_event(\"AGENT\", \"ExecutorPrep\", \"Preparing code execution\")\n",
    "    tool_call_message = AIMessage(\n",
    "        content=\"\", \n",
    "        tool_calls=[{\n",
    "            'name': 'execute_pyspark_code', \n",
    "            'args': {'code': state.get('pyspark_code')}, \n",
    "            'id': f'exec_{datetime.now().isoformat()}'\n",
    "        }]\n",
    "    )\n",
    "    return {\"messages\": [tool_call_message]}\n",
    "\n",
    "def after_review_decider(state: AgentState):\n",
    "    feedback = state.get('review_feedback', '')\n",
    "    if \"APPROVED\" in feedback.upper():\n",
    "        log_event(\"GRAPH\", \"Router\", \"Code approved → Execution\")\n",
    "        return \"prepare_for_execution\"\n",
    "    else:\n",
    "        log_event(\"GRAPH\", \"Router\", f\"Code rejected → Revision (attempt {state.get('retry_count', 0) + 1})\")\n",
    "        return \"revise_code\"\n",
    "\n",
    "def after_execution_decider(state: AgentState):\n",
    "    last_message = state['messages'][-1]\n",
    "    execution_result = json.loads(last_message.content)\n",
    "    \n",
    "    if execution_result[\"status\"] == \"success\":\n",
    "        log_event(\"GRAPH\", \"Router\", \"Execution successful → Complete\")\n",
    "        return END\n",
    "    elif state.get('retry_count', 0) < 3:\n",
    "        log_event(\"GRAPH\", \"Router\", \"Execution failed → Retry\")\n",
    "        state['execution_error'] = execution_result[\"error\"]\n",
    "        return \"code_generator\"\n",
    "    else:\n",
    "        log_event(\"ERROR\", \"Router\", \"Max retries exceeded → Abort\")\n",
    "        return END\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"planner\", planner_agent)\n",
    "workflow.add_node(\"code_generator\", code_generator_agent)\n",
    "workflow.add_node(\"code_reviewer\", code_reviewer_agent)\n",
    "workflow.add_node(\"revise_code_node\", lambda state: {\"retry_count\": state.get('retry_count', 0) + 1})\n",
    "workflow.add_node(\"prepare_for_execution\", prepare_for_execution)\n",
    "workflow.add_node(\"executor\", tool_node)\n",
    "\n",
    "workflow.set_entry_point(\"planner\")\n",
    "workflow.add_edge(\"planner\", \"code_generator\")\n",
    "workflow.add_edge(\"code_generator\", \"code_reviewer\")\n",
    "workflow.add_conditional_edges(\"code_reviewer\", after_review_decider, {\n",
    "    \"prepare_for_execution\": \"prepare_for_execution\", \n",
    "    \"revise_code\": \"revise_code_node\"\n",
    "})\n",
    "workflow.add_edge(\"revise_code_node\", \"code_generator\")\n",
    "workflow.add_edge(\"prepare_for_execution\", \"executor\")\n",
    "workflow.add_conditional_edges(\"executor\", after_execution_decider, {\n",
    "    \"code_generator\": \"code_generator\", \n",
    "    \"__end__\": END\n",
    "})\n",
    "\n",
    "app = workflow.compile()\n",
    "log_event(\"INFO\", \"Setup\", \"Enhanced LangGraph pipeline compiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac837680",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_workflow_diagram():\n",
    "    \"\"\"Create a text-based workflow diagram as fallback for visualization issues\"\"\"\n",
    "    diagram = \"\"\"\n",
    "    ┌─────────────┐\n",
    "    │   PLANNER   │\n",
    "    │   AGENT     │\n",
    "    └─────┬───────┘\n",
    "          │\n",
    "          ▼\n",
    "    ┌─────────────┐\n",
    "    │    CODE     │\n",
    "    │ GENERATOR   │\n",
    "    └─────┬───────┘\n",
    "          │\n",
    "          ▼\n",
    "    ┌─────────────┐\n",
    "    │    CODE     │\n",
    "    │  REVIEWER   │\n",
    "    └─────┬───────┘\n",
    "          │\n",
    "          ▼\n",
    "    ┌─────────────┐    ┌─────────────┐\n",
    "    │  APPROVED?  │───►│   REVISE    │\n",
    "    │             │    │    CODE     │\n",
    "    └─────┬───────┘    └─────┬───────┘\n",
    "          │                  │\n",
    "          ▼                  │\n",
    "    ┌─────────────┐          │\n",
    "    │  EXECUTE    │◄─────────┘\n",
    "    │    CODE     │\n",
    "    └─────┬───────┘\n",
    "          │\n",
    "          ▼\n",
    "    ┌─────────────┐\n",
    "    │  SUCCESS?   │\n",
    "    │             │\n",
    "    └─────────────┘\n",
    "    \"\"\"\n",
    "    print(\"=== LANGGRAPH WORKFLOW DIAGRAM ===\")\n",
    "    print(diagram)\n",
    "\n",
    "try:\n",
    "    # Try to create visual graph with improved error handling\n",
    "    try:\n",
    "        # First try with mermaid text output (safer fallback)\n",
    "        mermaid_code = app.get_graph().draw_mermaid()\n",
    "        print(\"=== WORKFLOW MERMAID DIAGRAM ===\")\n",
    "        print(mermaid_code)\n",
    "    except Exception as mermaid_error:\n",
    "        log_event(\"INFO\", \"Visualization\", f\"Mermaid generation failed: {mermaid_error}\")\n",
    "        \n",
    "    # Try PNG generation with better error handling\n",
    "    try:\n",
    "        from langgraph.graph.graph import CompiledGraph\n",
    "        if hasattr(app.get_graph(), 'draw_ascii'):\n",
    "            ascii_graph = app.get_graph().draw_ascii()\n",
    "            print(\"=== WORKFLOW ASCII DIAGRAM ===\")\n",
    "            print(ascii_graph)\n",
    "        else:\n",
    "            create_workflow_diagram()\n",
    "    except Exception as png_error:\n",
    "        log_event(\"INFO\", \"Visualization\", f\"PNG visualization unavailable: {png_error}\")\n",
    "        create_workflow_diagram()\n",
    "        \n",
    "except Exception as e:\n",
    "    log_event(\"INFO\", \"Visualization\", \"Using text-based workflow diagram\")\n",
    "    create_workflow_diagram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ec4c76",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## Cell 9: Execute Bronze to Silver Transformations\n",
    "\n",
    "This cell initiates the first major phase of the Medallion pipeline: transforming raw data from the Bronze layer to a cleaned and standardized Silver layer. It iterates through a list of predefined transformation tasks, each describing requirements for cleansing and validating data for specific tables (customers, transactions, accounts, opportunities). The execute_pipeline_task function orchestrates the agentic workflow for each task.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf5c124",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def execute_pipeline_task(task_description):\n",
    "    task_name = task_description.strip().splitlines()[0][:50]\n",
    "    log_event(\"INFO\", \"Pipeline\", f\"EXECUTING: {task_name}\")\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=\"Initialize transformation pipeline\")], \n",
    "        \"current_task\": task_description, \n",
    "        \"retry_count\": 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        for state_update in app.stream(initial_state, {\"recursion_limit\": 20}):\n",
    "            node_name = list(state_update.keys())[0]\n",
    "            log_event(\"GRAPH\", \"Flow\", f\"Completed: {node_name}\")\n",
    "        \n",
    "        log_event(\"INFO\", \"Pipeline\", f\"COMPLETED: {task_name}\")\n",
    "    except Exception as e:\n",
    "        log_event(\"ERROR\", \"Pipeline\", f\"Task failed: {task_name} - {str(e)}\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "log_event(\"INFO\", \"PIPELINE\", \"===== BRONZE → SILVER TRANSFORMATIONS =====\")\n",
    "\n",
    "bronze_to_silver_transformations = [\n",
    "    \"\"\"CUSTOMER DATA CLEANSING: ops_bronze.customers_raw → ops_silver.customers_cleaned\n",
    "    \n",
    "    REQUIREMENTS:\n",
    "    - Remove duplicate customer_id records (keep first occurrence)  \n",
    "    - Fill null 'name' values with 'Unknown Customer'\n",
    "    - Validate email format using regex pattern '.+@.+\\\\..+' \n",
    "    - Set invalid emails to null\n",
    "    - Convert join_date string to proper DateType\n",
    "    - Filter out records with null email OR null address\n",
    "    - Add data quality flags for tracking\"\"\",\n",
    "    \n",
    "    \"\"\"TRANSACTION DATA STANDARDIZATION: ops_bronze.transactions_raw → ops_silver.transactions_cleaned\n",
    "    \n",
    "    REQUIREMENTS:\n",
    "    - Deduplicate on transaction_id (keep first record)\n",
    "    - Clean amount field: remove '$' prefix and convert to Decimal(10,2)\n",
    "    - Filter out negative quantity transactions\n",
    "    - Filter out null or zero amount transactions  \n",
    "    - Restrict to customer_id <= 100 (valid customers only)\n",
    "    - Convert transaction_date to DateType\n",
    "    - Add calculated fields for analysis\"\"\",\n",
    "    \n",
    "    \"\"\"ACCOUNT DATA NORMALIZATION: ops_bronze.accounts_raw → ops_silver.accounts_cleaned\n",
    "    \n",
    "    REQUIREMENTS:\n",
    "    - Deduplicate by account_id, preserving first record\n",
    "    - Standardize industry values to uppercase\n",
    "    - Map 'TECH' industry to 'TECHNOLOGY' \n",
    "    - Replace null industry with 'NOT_SPECIFIED'\n",
    "    - Validate account_id format consistency\n",
    "    - Add data lineage tracking columns\"\"\",\n",
    "    \n",
    "    \"\"\"OPPORTUNITY DATA VALIDATION: ops_bronze.opportunities_raw → ops_silver.opportunities_cleaned\n",
    "    \n",
    "    REQUIREMENTS:\n",
    "    - Cast opportunity value to Decimal(14,2) with proper handling\n",
    "    - Filter out opportunities with value <= 0\n",
    "    - Validate account_id matches pattern 'ACC###'\n",
    "    - Convert close_date string to DateType\n",
    "    - Standardize stage values with proper casing\n",
    "    - Add business rule validation flags\"\"\"\n",
    "]\n",
    "\n",
    "for task in bronze_to_silver_transformations:\n",
    "    execute_pipeline_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a96a95",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## Cell 10: Execute Silver to Gold Aggregations \n",
    "\n",
    "This cell performs the second major phase of the Medallion pipeline: aggregating and enriching data from the Silver layer into the Gold layer. It defines and executes tasks for creating analytical tables like customer_spending, account_performance, and monthly_sales_summary. These aggregations prepare the data for business intelligence and reporting.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42cd24b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "log_event(\"INFO\", \"PIPELINE\", \"===== SILVER → GOLD AGGREGATIONS =====\")\n",
    "\n",
    "silver_to_gold_aggregations = [\n",
    "    \"\"\"CUSTOMER SPENDING ANALYTICS: Create ops_gold.customer_spending\n",
    "    \n",
    "    REQUIREMENTS:\n",
    "    - Join ops_silver.customers_cleaned with ops_silver.transactions_cleaned\n",
    "    - Group by customer_id and customer name\n",
    "    - Calculate total_spent (sum of amounts) with null handling\n",
    "    - Calculate total_transactions (count of transactions)\n",
    "    - Calculate average_transaction_value \n",
    "    - Add customer spending tier classification\n",
    "    - Order by total_spent descending for performance\"\"\",\n",
    "    \n",
    "    \"\"\"ACCOUNT PERFORMANCE METRICS: Create ops_gold.account_performance  \n",
    "    \n",
    "    REQUIREMENTS:\n",
    "    - Join ops_silver.accounts_cleaned with ops_silver.opportunities_cleaned\n",
    "    - Group by account_id, account_name, and industry\n",
    "    - Calculate total_pipeline_value (sum of all opportunity values)\n",
    "    - Calculate won_value (sum where stage = 'Closed Won')\n",
    "    - Calculate win_rate as percentage of closed won vs total closed\n",
    "    - Count open_opportunities (non-closed stages)\n",
    "    - Add performance ranking within industry\"\"\",\n",
    "    \n",
    "    \"\"\"MONTHLY SALES TRENDS: Create ops_gold.monthly_sales_summary\n",
    "    \n",
    "    REQUIREMENTS:\n",
    "    - Source from ops_silver.transactions_cleaned\n",
    "    - Extract year-month from transaction_date as 'YYYY-MM' format\n",
    "    - Group by month period  \n",
    "    - Calculate monthly_revenue (sum of amounts)\n",
    "    - Calculate monthly_transaction_count\n",
    "    - Calculate average_transaction_size per month\n",
    "    - Add month-over-month growth calculations\n",
    "    - Order chronologically for time series analysis\"\"\"\n",
    "]\n",
    "\n",
    "for task in silver_to_gold_aggregations:\n",
    "    execute_pipeline_task(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368e267",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "log_event(\"INFO\", \"PIPELINE\", \"===== BUSINESS INTELLIGENCE DASHBOARD =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a91e03",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "## Cell 12: Generate and Validate Dashboard Visualizations\n",
    "\n",
    "This cell leverages the bi_agent (which uses the Notebook_visualization tool) to generate key business intelligence dashboard components. It then performs final data validation on the Gold layer tables by displaying record counts and top N rows, ensuring the pipeline's output is as expected.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bcaf0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dashboard_prompt = \"\"\"You are a Business Intelligence specialist. Create executive dashboard visualizations using the create_notebook_visualization tool.\n",
    "\n",
    "DASHBOARD REQUIREMENTS:\n",
    "1. Customer Analysis: Top 5 customers by total spending (bar chart from ops_gold.customer_spending)\n",
    "2. Industry Performance: Pipeline value distribution by industry (bar chart from ops_gold.account_performance)  \n",
    "3. Revenue Trends: Monthly revenue progression over time (line chart from ops_gold.monthly_sales_summary)\n",
    "\n",
    "Execute all three visualizations to complete the executive dashboard.\"\"\"\n",
    "\n",
    "bi_agent = llm.bind_tools(all_tools)\n",
    "dashboard_response = bi_agent.invoke(dashboard_prompt)\n",
    "\n",
    "if dashboard_response.tool_calls:\n",
    "    for tool_call in dashboard_response.tool_calls:\n",
    "        tool_function = {t.name: t for t in all_tools}[tool_call['name']]\n",
    "        result = tool_function.invoke(tool_call['args'])\n",
    "        log_event(\"INFO\", \"Dashboard\", f\"Visualization created: {tool_call['args'].get('title', 'Unknown')}\")\n",
    "else:\n",
    "    log_event(\"ERROR\", \"Dashboard\", \"Failed to generate dashboard visualizations\")\n",
    "\n",
    "log_event(\"INFO\", \"VALIDATION\", \"===== FINAL DATA VALIDATION =====\")\n",
    "\n",
    "validation_tables = [\n",
    "    \"ops_gold.customer_spending\",\n",
    "    \"ops_gold.account_performance\", \n",
    "    \"ops_gold.monthly_sales_summary\"\n",
    "]\n",
    "\n",
    "for table in validation_tables:\n",
    "    try:\n",
    "        print(f\"\\n=== FINAL VALIDATION: {table} ===\")\n",
    "        df = spark.table(table)\n",
    "        print(f\"Record Count: {df.count()}\")\n",
    "        if \"customer_spending\" in table:\n",
    "            display(df.orderBy(col(\"total_spent\").desc()).limit(10))\n",
    "        elif \"account_performance\" in table:\n",
    "            display(df.orderBy(col(\"total_pipeline_value\").desc()).limit(10))\n",
    "        else:\n",
    "            display(df.orderBy(\"month\").limit(12))\n",
    "    except Exception as e:\n",
    "        log_event(\"ERROR\", \"Validation\", f\"Failed to validate {table}: {str(e)}\")\n",
    "\n",
    "log_event(\"INFO\", \"COMPLETION\", \"===== MEDALLION PIPELINE EXECUTION COMPLETE =====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f548d91",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n🎉 SUCCESS: Databricks Medallion Architecture pipeline completed successfully!\")\n",
    "print(\"✅ All Bronze → Silver → Gold transformations executed\")\n",
    "print(\"✅ Business Intelligence dashboard components created\")\n",
    "print(\"✅ Data quality validation completed\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
