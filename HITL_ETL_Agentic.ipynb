{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# Human-in-the-Loop (HITL) Agentic Medallion Pipeline\n",
    "\n",
    "_An advanced data transformation workflow leveraging a multi-agent system to automate ETL development with robust human oversight._\n",
    "\n",
    "This project demonstrates a sophisticated data engineering pipeline that utilizes a team of specialized AI agents to automate the creation of PySpark transformations within a Medallion architecture. The agents collaboratively handle the entire development lifecycle—from planning and coding to testing—for processing data through Bronze, Silver, and Gold layers.\n",
    "\n",
    "The core principle is **Human-in-the-Loop (HITL)**. Instead of executing code directly, the final agent action is to commit the generated PySpark logic and its corresponding `pytest` suite to a Git repository and open a pull request. This workflow ensures that all AI-generated code is subject to mandatory human review and approval before being merged and deployed, blending the speed of automation with the quality and control of expert oversight.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1: Installing Dependencies\n",
    "This cell uses the `%pip` magic command to install necessary Python libraries. Key libraries include `langchain` for building the agentic workflow, `databricks-langchain` for integration with Databricks, `langgraph` for creating the stateful graph, `gitpython` for programmatic Git operations, `pytest` for testing the generated code, and `requests` for making API calls to GitHub. `dbutils.library.restartPython()` is then called to ensure the newly installed libraries are available in the notebook's environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d440a540-4d9f-4f31-b3e9-d73e82d4adc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Added gitpython, pytest, and requests for version control, testing, and API calls.\n",
    "%pip install langchain langchain-community langchain-core databricks-langchain langgraph playwright Pillow gitpython pytest requests\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: Configuring LangSmith Tracing\n",
    "This cell sets up environment variables to configure LangSmith, a platform for monitoring and debugging LangChain applications. It enables tracing (`LANGCHAIN_TRACING_V2`), sets the API endpoint, and requires a user-provided API key. A project name is also set to organize the traces, which is crucial for observing the complex interactions between agents and tools in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c40d30-4ce3-48f0-ba72-bdb2590ea6b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# No changes here. LangSmith is crucial for tracing this more complex workflow.\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \" \"  # <--- REPLACE WITH YOUR KEY\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Databricks - Medallion Pipeline v2.1 (Rejection)\"\n",
    "\n",
    "print(f\"LangSmith configured. Project: '{os.environ['LANGCHAIN_PROJECT']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 3: Git and GitHub Configuration\n",
    "This cell establishes the configuration parameters required for the agent to interact with a GitHub repository. It defines the local path to the git repository (`GIT_REPO_PATH`), and sets up placeholder variables for a GitHub Personal Access Token (`GITHUB_API_TOKEN`), the repository owner, and the repository name. These variables are essential for the agent's tool that creates pull requests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40623f04-6f31-408d-8048-cecab277d0a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# This path should now simply point to the root of the current repository.\n",
    "# In Databricks, you can often use a relative path.\n",
    "GIT_REPO_PATH = \".\" \n",
    "\n",
    "# These variables are still needed for the GitHub API call to create the PR.\n",
    "GITHUB_API_TOKEN = \"\" # Replace with a secret in production\n",
    "GITHUB_REPO_OWNER = \"your github username\"\n",
    "GITHUB_REPO_NAME = \"HITL\"\n",
    "\n",
    "# The user name and email are no longer needed here, as Databricks will handle it.\n",
    "GIT_USERNAME = \"Your github username\"\n",
    "GIT_EMAIL = \"Your Github linked email\"\n",
    "\n",
    "print(f\"Git operations will run in the context of the current repository: '{os.path.abspath(GIT_REPO_PATH)}'\")\n",
    "print(\"Username and email are configured in Databricks User Settings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 4: Importing Core Libraries\n",
    "This cell imports all the necessary Python classes, functions, and modules that will be used throughout the notebook. This includes components from `langchain` for building the AI agents, `pyspark` for data manipulation, `git` for version control, and standard libraries like `os`, `json`, and `datetime` for various utility tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1462e35c-ef50-4092-a49c-dacc2b486659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary classes and functions\n",
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "from typing import Annotated, TypedDict, List, Optional\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, when, lit, md5, concat_ws, expr, to_date, upper, sum as _sum, avg as _avg, count as _count, date_format, year, month, current_timestamp, to_json, struct\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "import subprocess\n",
    "import git\n",
    "import requests\n",
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"All dependencies imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 5: Initializing Spark and Databases\n",
    "This cell handles the setup of the core data processing environment. It initializes the `SparkSession`, which is the entry point for any Spark functionality. It also configures the Large Language Model (LLM) using `ChatDatabricks`. Crucially, it creates the SQL databases for the Medallion architecture (`ops_bronze`, `ops_silver`, `ops_gold`) and a centralized `Rejected` database, which includes a `rejected_rows` table to store data that fails validation at any stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb29ed8a-2c2c-4e95-9485-40f90f7d24a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_event(level, source, message):\n",
    "    colors = {\"AGENT\": \"\\033[94m\", \"TOOL\": \"\\033[93m\", \"INFO\": \"\\033[92m\", \"ERROR\": \"\\033[91m\", \"GRAPH\": \"\\033[95m\"}\n",
    "    reset_color = \"\\033[0m\"\n",
    "    timestamp = datetime.now().strftime(\"%H:%M:%S.%f\")[:-3]\n",
    "    print(f\"{timestamp} | {colors.get(level, '')}{level:<7}{reset_color} | {colors.get(level, '')}[{source}]{reset_color} {message}\")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AgenticDataPipeline\").getOrCreate()\n",
    "llm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-1-405b-instruct\", temperature=0.0, max_tokens=4096)\n",
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS ops_bronze\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS ops_silver\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS ops_gold\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS Rejected\") # Centralized DB for all rejected data\n",
    "\n",
    "# A structured table to hold all rejected rows from any pipeline step\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Rejected.rejected_rows (\n",
    "    rejection_timestamp TIMESTAMP,\n",
    "    pipeline_step STRING,\n",
    "    source_table STRING,\n",
    "    rejection_reason STRING,\n",
    "    data_payload STRING\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "log_event(\"INFO\", \"Setup\", \"Environment initialized with central 'Rejected' database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 6: Generating Bronze Layer Data\n",
    "This function, `create_bronze_data()`, synthesizes raw data to populate the `ops_bronze` tables. It intentionally introduces common data quality issues such as null values, duplicates, and incorrect formats across customer, transaction, account, and opportunity tables. This simulated raw data serves as the starting point for the agentic ETL pipeline to process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce0db8f-cce9-4513-9341-50d5d2ceed09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_bronze_data():\n",
    "    log_event(\"INFO\", \"DataGen\", \"Generating Bronze layer data...\")\n",
    "    \n",
    "    customer_data = []\n",
    "    for i in range(1, 101):\n",
    "        name = f\"FName{i} LName{i}\" if random.random() > 0.1 else None\n",
    "        email = f\"fname{i}.lname{i}@email.com\" if random.random() > 0.1 else \"invalid-email\"\n",
    "        address = f\"{i*10} Main St\" if random.random() > 0.05 else None\n",
    "        customer_data.append((i, name, email, address, f\"2023-01-{random.randint(10, 28)}\"))\n",
    "    \n",
    "    customer_data.extend([\n",
    "        (1, 'John Doe', 'john.doe@email.com', '123 Elm St', '2023-01-15'), \n",
    "        (3, 'Peter Jones', 'peter.jones.dup@email.com', '789 Pine Ln', '2023-01-17')\n",
    "    ])\n",
    "    spark.createDataFrame(customer_data, [\"customer_id\", \"name\", \"email\", \"address\", \"join_date\"]).write.mode(\"overwrite\").saveAsTable(\"ops_bronze.customers_raw\")\n",
    "\n",
    "    transactions_data = []\n",
    "    for i in range(1, 201):\n",
    "        cust_id = random.randint(1, 110)\n",
    "        qty = random.randint(-5, 50)\n",
    "        amount = f\"${random.uniform(5, 1000):.2f}\" if random.random() > 0.05 else None\n",
    "        transactions_data.append((100+i, cust_id, qty, amount, f\"2023-02-{random.randint(1, 28)}\"))\n",
    "    # Add a transaction for a customer who won't exist in the cleaned customer table\n",
    "    transactions_data.append((999, 105, 10, \"$50.00\", \"2023-02-20\"))\n",
    "    spark.createDataFrame(transactions_data, [\"transaction_id\", \"customer_id\", \"quantity\", \"amount\", \"transaction_date\"]).write.mode(\"overwrite\").saveAsTable(\"ops_bronze.transactions_raw\")\n",
    "\n",
    "    accounts_data = [('ACC{:03d}'.format(i), f'GlobalCorp-{i}', random.choice(['Technology', 'Healthcare', 'Finance', 'TECH', None]), random.choice(['USA', 'UK', 'Germany'])) for i in range(1, 51)]\n",
    "    accounts_data.append(('ACC001', 'Global Corporation', 'Tech', 'USA'))\n",
    "    spark.createDataFrame(accounts_data, [\"account_id\", \"account_name\", \"industry\", \"region\"]).write.mode(\"overwrite\").saveAsTable(\"ops_bronze.accounts_raw\")\n",
    "    \n",
    "    opportunities_data = [(f'OPP{i:03d}', f\"ACC{random.randint(1, 55):03d}\", random.randint(-1000, 200000), '2024-07-15', random.choice(['Closed Won', 'Negotiation', 'Proposal', 'Qualification', 'Closed Lost'])) for i in range(1, 151)]\n",
    "    spark.createDataFrame(opportunities_data, [\"opportunity_id\", \"account_id\", \"value\", \"close_date\", \"stage\"]).write.mode(\"overwrite\").saveAsTable(\"ops_bronze.opportunities_raw\")\n",
    "    \n",
    "    log_event(\"INFO\", \"DataGen\", \"Bronze data creation complete\")\n",
    "    display(spark.sql(\"SHOW TABLES IN ops_bronze\"))\n",
    "\n",
    "create_bronze_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 7: Defining Agent Tools\n",
    "This cell defines the set of tools the AI agents can use to perform actions. Each tool is a Python function decorated with `@tool`, which makes it discoverable by the LangChain agent. The tools include `get_table_info` to profile data, `run_pytest_tests` to validate generated code, `create_pull_request` to interact with GitHub, and `Notesbook_visualization` for dashboarding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccbf6f7b-cc43-45f0-9ced-882750ae01ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_table_info(table_name: str) -> str:\n",
    "    \"\"\"Provides comprehensive table profiling including schema, statistics, and data preview for transformation planning.\"\"\"\n",
    "    try:\n",
    "        log_event(\"TOOL\", \"get_table_info\", f\"Profiling: {table_name}\")\n",
    "        df = spark.table(table_name)\n",
    "        schema_str = \"\\n\".join([f\"- {field.name}: {str(field.dataType)}\" for field in df.schema.fields])\n",
    "        count = df.count()\n",
    "        preview = df.limit(3).toPandas().to_string()\n",
    "        return f\"TABLE: {table_name}\\nROW_COUNT: {count}\\n\\nSCHEMA:\\n{schema_str}\\n\\nSAMPLE_DATA:\\n{preview}\"\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def run_pytest_tests(code: str, tests: str) -> str:\n",
    "    \"\"\"Executes a pytest suite against provided PySpark code to ensure correctness. Returns success or failure with logs.\"\"\"\n",
    "    log_event(\"TOOL\", \"run_pytest_tests\", \"Validating generated code with pytest...\")\n",
    "    try:\n",
    "        # Write code and tests to temporary files to be executed\n",
    "        code_path = os.path.join(GIT_REPO_PATH, \"temp_transform.py\")\n",
    "        test_path = os.path.join(GIT_REPO_PATH, \"test_temp_transform.py\")\n",
    "        with open(code_path, \"w\") as f:\n",
    "            f.write(code)\n",
    "        with open(test_path, \"w\") as f:\n",
    "            f.write(tests)\n",
    "\n",
    "        # Execute pytest using a subprocess to isolate the run\n",
    "        result = subprocess.run(\n",
    "            [\"pytest\", test_path],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            cwd=GIT_REPO_PATH\n",
    "        )\n",
    "\n",
    "        # Clean up temporary files\n",
    "        os.remove(code_path)\n",
    "        os.remove(test_path)\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            log_event(\"TOOL\", \"run_pytest_tests\", \"SUCCESS: All tests passed.\")\n",
    "            return json.dumps({\"status\": \"success\", \"report\": result.stdout})\n",
    "        else:\n",
    "            log_event(\"ERROR\", \"run_pytest_tests\", f\"FAILURE: Tests failed. Return code: {result.returncode}\")\n",
    "            return json.dumps({\"status\": \"error\", \"error\": result.stdout + result.stderr})\n",
    "    except Exception as e:\n",
    "        error_msg = f\"PYTEST_EXECUTION_ERROR: {str(e)}\"\n",
    "        log_event(\"ERROR\", \"run_pytest_tests\", error_msg)\n",
    "        return json.dumps({\"status\": \"error\", \"error\": error_msg})\n",
    "\n",
    "@tool\n",
    "def create_pull_request(task_name: str, code: str, tests: str, plan: str) -> str:\n",
    "    \"\"\"Creates a new branch, commits the code and tests, and opens a GitHub pull request for human review.\"\"\"\n",
    "    log_event(\"TOOL\", \"create_pull_request\", f\"Starting Git workflow for: {task_name}\")\n",
    "    if not GITHUB_API_TOKEN:\n",
    "        return \"ERROR: GitHub API Token is not configured. Cannot create pull request.\"\n",
    "    try:\n",
    "        repo = git.Repo(GIT_REPO_PATH)\n",
    "        \n",
    "        # The config is now handled by Databricks User Settings, so these lines are not needed\n",
    "        # repo.config_writer().set_value(\"user\", \"name\", GIT_USERNAME).release()\n",
    "        # repo.config_writer().set_value(\"user\", \"email\", GIT_EMAIL).release()\n",
    "\n",
    "        # Create a unique branch name\n",
    "        branch_name = f\"feature/{task_name.lower().replace(' ', '-').replace(':', '')}-{int(time.time())}\"\n",
    "        \n",
    "        repo.git.checkout('HEAD', b=branch_name)\n",
    "        log_event(\"TOOL\", \"Git\", f\"Created and checked out new branch: {branch_name}\")\n",
    "\n",
    "        # Create file paths in designated directories\n",
    "        transform_dir = os.path.join(GIT_REPO_PATH, \"transforms\")\n",
    "        test_dir = os.path.join(GIT_REPO_PATH, \"tests\")\n",
    "        os.makedirs(transform_dir, exist_ok=True)\n",
    "        os.makedirs(test_dir, exist_ok=True)\n",
    "        \n",
    "        file_name = f\"{task_name.split('→')[0].split(':')[-1].strip().lower().replace(' ', '_')}.py\"\n",
    "        code_path = os.path.join(transform_dir, file_name)\n",
    "        test_path = os.path.join(test_dir, f\"test_{file_name}\")\n",
    "\n",
    "        with open(code_path, \"w\") as f: f.write(code)\n",
    "        with open(test_path, \"w\") as f: f.write(tests)\n",
    "        \n",
    "        repo.index.add([code_path, test_path])\n",
    "        repo.index.commit(f\"feat: Implement transformation for '{task_name}'\")\n",
    "        log_event(\"TOOL\", \"Git\", \"Committed new code and test files.\")\n",
    "\n",
    "        repo.git.push('--set-upstream', 'origin', branch_name)\n",
    "        log_event(\"TOOL\", \"Git\", \"Pushed new branch to remote origin.\")\n",
    "\n",
    "        # Create Pull Request via GitHub API\n",
    "        pr_title = f\"Agent-Generated Transformation: {task_name}\"\n",
    "        pr_body = f\"### Original Task\\n{task_name}\\n\\n### Agent's Plan\\n```\\n{plan}\\n```\\n\\nThis PR was generated automatically by the Databricks Agentic Pipeline. Please review the code and tests before merging.\"\n",
    "        \n",
    "        headers = {\"Authorization\": f\"token {GITHUB_API_TOKEN}\", \"Accept\": \"application/vnd.github.v3+json\"}\n",
    "        data = {\"title\": pr_title, \"head\": branch_name, \"base\": \"main\", \"body\": pr_body}\n",
    "        url = f\"https://api.github.com/repos/{GITHUB_REPO_OWNER}/{GITHUB_REPO_NAME}/pulls\"\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "        if response.status_code == 201:\n",
    "            pr_url = response.json()['html_url']\n",
    "            log_event(\"TOOL\", \"GitHub\", f\"SUCCESS: Pull Request created at {pr_url}\")\n",
    "            repo.git.checkout('main')\n",
    "            return f\"SUCCESS: Pull Request created at {pr_url}\"\n",
    "        else:\n",
    "            error_msg = f\"GitHub API Error: {response.status_code} - {response.text}\"\n",
    "            log_event(\"ERROR\", \"GitHub\", error_msg)\n",
    "            return error_msg\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"GIT_PR_ERROR: {str(e)}\"\n",
    "        log_event(\"ERROR\", \"Git\", error_msg)\n",
    "        return error_msg\n",
    "\n",
    "@tool\n",
    "def create_notebook_visualization(table_name: str, plot_type: str, x_col: str, y_col: str, title: str) -> str:\n",
    "    \"\"\"Creates dashboard visualizations. To be used after transformations are merged and executed.\"\"\"\n",
    "    try:\n",
    "        log_event(\"TOOL\", \"create_visualization\", f\"Creating: {title}\")\n",
    "        df = spark.table(table_name)\n",
    "        if plot_type == 'bar' and 'Top' in title:\n",
    "            df_display = df.orderBy(col(y_col).desc()).limit(5)\n",
    "        else:\n",
    "            df_display = df.orderBy(x_col)\n",
    "        print(f\"\\n=== DASHBOARD COMPONENT: {title} ===\")\n",
    "        display(df_display)\n",
    "        return f\"SUCCESS: '{title}' visualization created and displayed\"\n",
    "    except Exception as e:\n",
    "        return f\"VISUALIZATION_ERROR: {str(e)}\"\n",
    "\n",
    "all_tools = [get_table_info, run_pytest_tests, create_pull_request, create_notebook_visualization]\n",
    "tool_node = ToolNode(all_tools)\n",
    "\n",
    "print(\"All agent tools defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 8: Defining the Agentic Workflow and State\n",
    "This cell contains the core logic for the agentic system. It defines the `AgentState` class to manage the flow of information between steps. It then defines the different agents (`planner_agent`, `code_generator_agent`, `code_reviewer_agent`) and the decision-making logic (`after_review_decider`, `after_testing_decider`). Finally, it assembles these components into a `StateGraph` from the `langgraph` library, compiling the complete, multi-agent workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2e3af4a-356d-4f2d-b307-9346ff6ec4ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, lambda x, y: x + y]\n",
    "    current_task: str\n",
    "    plan: Optional[str]\n",
    "    pyspark_code: Optional[str]\n",
    "    pyspark_tests: Optional[str]\n",
    "    review_feedback: Optional[str]\n",
    "    test_results: Optional[str]\n",
    "    retry_count: int\n",
    "\n",
    "def planner_agent(state: AgentState):\n",
    "    log_event(\"AGENT\", \"Planner\", \"Creating transformation strategy and test plan\")\n",
    "    system_prompt = \"\"\"You are an expert data architect. Your job is to create a detailed, step-by-step technical plan for a PySpark transformation.\n",
    "    CRITICAL: You must also define a testing strategy. Describe the specific unit tests needed to validate the logic, including edge cases, null handling, and expected outputs.\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=f\"{system_prompt}\\n\\nTASK: {state['current_task']}\")])\n",
    "    return {\"messages\": [AIMessage(content=response.content, name=\"PlannerAgent\")], \"plan\": response.content}\n",
    "\n",
    "def code_generator_agent(state: AgentState):\n",
    "    log_event(\"AGENT\", \"CodeGenerator\", \"Generating PySpark code and pytest suite with rejection handling\")\n",
    "    plan = state['plan']\n",
    "    context_prompt = \"\"\n",
    "    if state.get('review_feedback') and \"APPROVED\" not in state['review_feedback'].upper():\n",
    "        context_prompt = f\"\\n\\nREVIEW FEEDBACK TO ADDRESS:\\n{state['review_feedback']}\"\n",
    "    elif state.get('test_results'):\n",
    "        test_feedback = json.loads(state['test_results'])\n",
    "        if test_feedback.get('status') == 'error':\n",
    "            context_prompt = f\"\\n\\nTESTS FAILED. FIX THE CODE. ERROR LOG:\\n{test_feedback.get('error')}\"\n",
    "\n",
    "    system_prompt = \"\"\"You are a senior PySpark developer specializing in robust, production-grade pipelines. Your task is to generate two distinct code blocks based on the plan.\n",
    "\n",
    "    **CRITICAL REQUIREMENT: REJECTION HANDLING**\n",
    "    For ANY transformation, you MUST capture all records that are filtered, dropped, or fail validation and write them to the `Rejected.rejected_rows` table. Do not silently drop data.\n",
    "\n",
    "    - **For Cleansing/Validation (Bronze -> Silver):**\n",
    "        - First, create a temporary view of the source data.\n",
    "        - Add columns to flag validation errors (e.g., `is_valid_email`, `has_positive_quantity`).\n",
    "        - Create a `reasons` array column concatenating all rejection reasons for a given row.\n",
    "        - Write all rows with one or more rejection reasons to `Rejected.rejected_rows`.\n",
    "        - The final transformation should only write the clean rows to the target Silver table.\n",
    "    - **For Joins/Aggregations (Silver -> Gold):**\n",
    "        - When joining tables (e.g., transactions to customers), you MUST capture records that do not find a match.\n",
    "        - Perform a **LEFT ANTI JOIN** to identify these \"orphan\" records from the left table.\n",
    "        - Write these orphan records to `Rejected.rejected_rows` with a clear reason (e.g., 'NO_MATCHING_CUSTOMER_RECORD').\n",
    "    - **Structure for Rejected Data:** Use the following format for writing to `Rejected.rejected_rows`:\n",
    "        - `rejection_timestamp`: `current_timestamp()`\n",
    "        - `pipeline_step`: A string literal describing the task (e.g., 'BRONZE_TO_SILVER_CUSTOMERS').\n",
    "        - `source_table`: The name of the source table.\n",
    "        - `rejection_reason`: The specific reason for failure.\n",
    "        - `data_payload`: The entire rejected row converted to a JSON string using `to_json(struct(*))`.\n",
    "\n",
    "    **OUTPUT FORMAT:**\n",
    "    1.  A Python code block with the complete PySpark transformation logic, including the rejection handling described above.\n",
    "    2.  A Python code block with a complete `pytest` test suite. The tests must validate both the successful transformation AND the rejection logic.\n",
    "\n",
    "    Separate the two code blocks with the exact string `---PYTEST_SEPARATOR---`.\n",
    "    \"\"\"\n",
    "    user_prompt = f\"IMPLEMENTATION PLAN:\\n{plan}{context_prompt}\"\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=f\"{system_prompt}\\n\\n{user_prompt}\")])\n",
    "    \n",
    "    if \"---PYTEST_SEPARATOR---\" in response.content:\n",
    "        code, tests = response.content.split(\"---PYTEST_SEPARATOR---\", 1)\n",
    "    else: \n",
    "        code = response.content\n",
    "        tests = \"# Could not generate tests. Please review.\"\n",
    "\n",
    "    clean_code = code.strip().replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
    "    clean_tests = tests.strip().replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "    return {\n",
    "        \"pyspark_code\": clean_code, \n",
    "        \"pyspark_tests\": clean_tests, \n",
    "        \"test_results\": None, \n",
    "        \"review_feedback\": None\n",
    "    }\n",
    "\n",
    "def code_reviewer_agent(state: AgentState):\n",
    "    log_event(\"AGENT\", \"CodeReviewer\", \"Reviewing code, tests, and rejection logic\")\n",
    "    system_prompt = \"\"\"You are a senior data engineering QA lead. Review the provided PySpark code AND the `pytest` test suite.\n",
    "    \n",
    "    REVIEW CRITERIA:\n",
    "    - Code correctness and adherence to the plan.\n",
    "    - **Crucially, does the code correctly implement the rejection handling logic for all specified cases (validation errors, failed joins)?**\n",
    "    - Test coverage: Do the tests adequately cover both successful transformations and the rejection logic?\n",
    "    - Production readiness and best practices.\n",
    "    \n",
    "    RESPONSE FORMAT:\n",
    "    - If all criteria are met: respond with exactly \"APPROVED\".\n",
    "    - Otherwise: respond with \"REJECTION_REASON: [specific, actionable feedback]\".\n",
    "    \"\"\"\n",
    "    user_prompt = f\"ORIGINAL PLAN:\\n{state['plan']}\\n\\nPYSPARK CODE:\\n```python\\n{state['pyspark_code']}\\n```\\n\\nPYTEST SUITE:\\n```python\\n{state['pyspark_tests']}\\n```\"\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=f\"{system_prompt}\\n\\n{user_prompt}\")])\n",
    "    return {\"review_feedback\": response.content}\n",
    "\n",
    "def prepare_for_testing(state: AgentState):\n",
    "    log_event(\"AGENT\", \"TestPrep\", \"Preparing to run automated tests\")\n",
    "    return {\"messages\": [AIMessage(content=\"\", tool_calls=[{\n",
    "        'name': 'run_pytest_tests', \n",
    "        'args': {'code': state['pyspark_code'], 'tests': state['pyspark_tests']}, \n",
    "        'id': f'test_{datetime.now().isoformat()}'\n",
    "    }])]}\n",
    "\n",
    "def prepare_for_pr(state: AgentState):\n",
    "    log_event(\"AGENT\", \"PRPrep\", \"Preparing to create pull request\")\n",
    "    task_name = state['current_task'].strip().splitlines()[0]\n",
    "    return {\"messages\": [AIMessage(content=\"\", tool_calls=[{\n",
    "        'name': 'create_pull_request',\n",
    "        'args': {\n",
    "            'task_name': task_name,\n",
    "            'code': state['pyspark_code'],\n",
    "            'tests': state['pyspark_tests'],\n",
    "            'plan': state['plan']\n",
    "        },\n",
    "        'id': f'pr_{datetime.now().isoformat()}'\n",
    "    }])]}\n",
    "\n",
    "def after_review_decider(state: AgentState):\n",
    "    feedback = state.get('review_feedback', '')\n",
    "    if \"APPROVED\" in feedback.upper():\n",
    "        log_event(\"GRAPH\", \"Router\", \"Code approved → Running Tests\")\n",
    "        return \"prepare_for_testing\"\n",
    "    else:\n",
    "        log_event(\"GRAPH\", \"Router\", f\"Code rejected → Revision (attempt {state.get('retry_count', 0) + 1})\")\n",
    "        return \"revise_code\"\n",
    "\n",
    "def after_testing_decider(state: AgentState):\n",
    "    last_message = state['messages'][-1]\n",
    "    test_results_json = last_message.content\n",
    "    test_results = json.loads(test_results_json)\n",
    "\n",
    "    # This state update is now handled by the 'revise_code' node\n",
    "    # state['test_results'] = test_results_json \n",
    "    \n",
    "    if test_results.get(\"status\") == \"success\":\n",
    "        log_event(\"GRAPH\", \"Router\", \"Tests passed → Creating Pull Request\")\n",
    "        return \"prepare_for_pr\"\n",
    "    # NEW: Check the retry count here before deciding to revise again\n",
    "    elif state.get('retry_count', 0) < 3:\n",
    "        log_event(\"GRAPH\", \"Router\", \"Tests failed → Revising Code\")\n",
    "        # Route to 'revise_code' to increment the counter correctly\n",
    "        return \"revise_code\"\n",
    "    else:\n",
    "        log_event(\"ERROR\", \"Router\", \"Max retries exceeded on test failures → Abort\")\n",
    "        return END\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"planner\", planner_agent)\n",
    "workflow.add_node(\"code_generator\", code_generator_agent)\n",
    "workflow.add_node(\"code_reviewer\", code_reviewer_agent)\n",
    "workflow.add_node(\"revise_code\", lambda state: {\"retry_count\": state.get('retry_count', 0) + 1, \"messages\": []})\n",
    "workflow.add_node(\"prepare_for_testing\", prepare_for_testing)\n",
    "workflow.add_node(\"test_runner\", ToolNode([run_pytest_tests]))\n",
    "workflow.add_node(\"prepare_for_pr\", prepare_for_pr)\n",
    "workflow.add_node(\"pr_creator\", ToolNode([create_pull_request]))\n",
    "\n",
    "workflow.set_entry_point(\"planner\")\n",
    "workflow.add_edge(\"planner\", \"code_generator\")\n",
    "workflow.add_edge(\"code_generator\", \"code_reviewer\")\n",
    "workflow.add_conditional_edges(\"code_reviewer\", after_review_decider, {\n",
    "    \"prepare_for_testing\": \"prepare_for_testing\",\n",
    "    \"revise_code\": \"revise_code\"\n",
    "})\n",
    "workflow.add_edge(\"revise_code\", \"code_generator\")\n",
    "workflow.add_edge(\"prepare_for_testing\", \"test_runner\")\n",
    "workflow.add_conditional_edges(\"test_runner\", after_testing_decider, {\n",
    "    \"prepare_for_pr\": \"prepare_for_pr\",\n",
    "    \"revise_code\": \"revise_code\", # Corrected: Route to revise_code on failure\n",
    "    \"__end__\": END\n",
    "})\n",
    "workflow.add_edge(\"prepare_for_pr\", \"pr_creator\")\n",
    "workflow.add_edge(\"pr_creator\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "log_event(\"INFO\", \"Setup\", \"HITL-enabled LangGraph pipeline compiled successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 9: Visualizing the Agentic Workflow\n",
    "This cell generates a Mermaid diagram script for the compiled agentic workflow. The `app.get_graph().draw_mermaid()` function inspects the compiled graph object and outputs a text-based representation. When this cell is run in a Databricks notebook, the output is automatically rendered as a visual flowchart, making it easy to understand the relationships and paths between the different nodes (agents and tools) in the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efd208aa-3c8e-445f-91f1-5c3c14901752",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This is the most reliable method as it has no external dependencies.\n",
    "# The notebook will render the text output into a visual diagram.\n",
    "\n",
    "print(\"--- Agentic Workflow Graph (Mermaid Diagram) ---\")\n",
    "mermaid_diagram = app.get_graph().draw_mermaid()\n",
    "print(mermaid_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 10: Executing Bronze-to-Silver Pipeline Tasks\n",
    "This cell defines the main execution loop for the pipeline. The `execute_pipeline_task` function takes a natural language task description, initializes the agentic workflow with that task, and streams its execution. It then defines a list of \"Bronze to Silver\" transformation tasks (customer cleansing and transaction standardization) and iterates through them, triggering the agentic workflow for each one to generate and propose a solution via a pull request.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb4dea94-441e-41af-8850-72f53ebd92b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def execute_pipeline_task(task_description):\n",
    "    task_name = task_description.strip().splitlines()[0][:50]\n",
    "    log_event(\"INFO\", \"Pipeline\", f\"INITIATING TASK: {task_name}\")\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [], \n",
    "        \"current_task\": task_description, \n",
    "        \"retry_count\": 0\n",
    "    }\n",
    "    \n",
    "    final_state = {}\n",
    "    try:\n",
    "        # Stream the workflow to get the final state\n",
    "        for state_update in app.stream(initial_state, {\"recursion_limit\": 25}):\n",
    "            node_name = list(state_update.keys())[0]\n",
    "            log_event(\"GRAPH\", \"Flow\", f\"Completed: {node_name}\")\n",
    "            final_state = state_update\n",
    "        \n",
    "        # NEW: Check if the workflow succeeded before trying to get the PR message\n",
    "        if 'pr_creator' in final_state:\n",
    "            pr_message = final_state.get('pr_creator', {}).get('messages', [{}])[-1].content\n",
    "            log_event(\"INFO\", \"Pipeline\", f\"SUCCESS: HITL workflow complete for {task_name}. Result: {pr_message}\")\n",
    "        else:\n",
    "            log_event(\"ERROR\", \"Pipeline\", f\"Workflow for '{task_name}' did not complete successfully and aborted before creating a PR.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_event(\"ERROR\", \"Pipeline\", f\"An unexpected error occurred during workflow for task '{task_name}': {str(e)}\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "log_event(\"INFO\", \"PIPELINE\", \"===== BRONZE → SILVER TRANSFORMATIONS (GENERATING PULL REQUESTS) =====\")\n",
    "\n",
    "bronze_to_silver_transformations = [\n",
    "    \"\"\"CUSTOMER DATA CLEANSING: ops_bronze.customers_raw → ops_silver.customers_cleaned\n",
    "    \n",
    "    REQUIREMENTS:\n",
    "    - Remove duplicate customer_id records (keep first occurrence).\n",
    "    - Fill null 'name' values with 'Unknown Customer'.\n",
    "    - Validate email format using regex pattern '.+@.+\\\\..+'.\n",
    "    - Convert join_date string to proper DateType.\n",
    "    - **REJECTION HANDLING**: Any row with an invalid email OR a null join_date must be written to `Rejected.rejected_rows` with a clear reason.\n",
    "    - The final silver table should only contain the clean, valid rows.\"\"\",\n",
    "    \n",
    "    \"\"\"TRANSACTION DATA STANDARDIZATION: ops_bronze.transactions_raw → ops_silver.transactions_cleaned\n",
    "    \n",
    "    REQUIREMENTS:\n",
    "    - Deduplicate on transaction_id (keep first record).\n",
    "    - Clean amount field: remove '$' prefix and convert to Decimal(10,2).\n",
    "    - Convert transaction_date to DateType.\n",
    "    - **REJECTION HANDLING**: Write any row to `Rejected.rejected_rows` if it has a negative quantity, a null/zero amount, OR if customer_id > 100.\"\"\"\n",
    "]\n",
    "\n",
    "for task in bronze_to_silver_transformations:\n",
    "    execute_pipeline_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 11: Executing Silver-to-Gold Pipeline Tasks\n",
    "This cell continues the pipeline by defining and executing the \"Silver to Gold\" aggregation tasks. Following the same pattern as the previous cell, it provides a high-level task description for creating a `customer_spending` analytics table. It then calls the `execute_pipeline_task` function, which once again invokes the full agentic workflow to plan, code, test, and create a pull request for this aggregation logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "228a7286-eabd-42ac-b95a-c47f47cc65d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_event(\"INFO\", \"PIPELINE\", \"===== SILVER → GOLD AGGREGATIONS (GENERATING PULL REQUESTS) =====\")\n",
    "\n",
    "silver_to_gold_aggregations = [\n",
    "    \"\"\"CUSTOMER SPENDING ANALYTICS: Create ops_gold.customer_spending\n",
    "    \n",
    "    REQUIREMENTS:\n",
    "    - Join `ops_silver.transactions_cleaned` with `ops_silver.customers_cleaned` on customer_id.\n",
    "    - Group by customer_id and customer name to calculate total_spent, total_transactions, etc.\n",
    "    - **REJECTION HANDLING**: Any transaction from `ops_silver.transactions_cleaned` that does not have a matching customer in `ops_silver.customers_cleaned` must be found using a LEFT ANTI JOIN and written to `Rejected.rejected_rows` with the reason 'NO_MATCHING_CUSTOMER'.\"\"\",\n",
    "]\n",
    "\n",
    "for task in silver_to_gold_aggregations:\n",
    "    execute_pipeline_task(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 12: Pausing for Human-in-the-Loop (HITL) Action\n",
    "This cell marks the critical hand-off point from the automated agent to a human reviewer. It prints a clear message informing the user that the agent's work is complete and that pull requests have been created in GitHub. It provides explicit next steps, instructing the user to navigate to their repository, review the generated code, and merge the PRs. This step is the \"Human-in-the-Loop\" part of the process, ensuring oversight before any code is deployed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd59b49e-26d4-42eb-a8e2-8c07136efac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_event(\"INFO\", \"PIPELINE\", \"===== HUMAN ACTION REQUIRED =====\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  All agentic workflows are complete.\")\n",
    "print(\"  Pull Requests have been generated in your GitHub repository.\")\n",
    "print(\"\\n  NEXT STEPS:\")\n",
    "print(\"  1. Go to your repository: \" + GIT_REPO_URL + \"/pulls\")\n",
    "print(\"  2. Review, comment on, and approve the generated PRs.\")\n",
    "print(\"  3. Merge the approved PRs into your main branch.\")\n",
    "print(\"  4. A separate CI/CD pipeline (e.g., GitHub Actions, Azure DevOps)\")\n",
    "print(\"     should then trigger to execute the new code against your Databricks workspace.\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nThe following cells are for validating the data AFTER that human-led process is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 13: Post-Merge Validation and Dashboarding\n",
    "This cell is designed to be run *after* the human has reviewed and merged the pull requests, and a separate CI/CD process has executed the new transformations. It simulates a business intelligence (BI) step by prompting another agent to use the `Notesbook_visualization` tool to build a dashboard. It also runs validation queries against the final Gold and Rejected tables to confirm that the pipeline ran as expected and to inspect the final outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0719bd2f-3c55-459b-8f9f-5f9089ca15a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dashboard_prompt = \"\"\"You are a Business Intelligence specialist. Create executive dashboard visualizations using the create_notebook_visualization tool.\n",
    "\n",
    "DASHBOARD REQUIREMENTS:\n",
    "1. Customer Analysis: Top 5 customers by total spending (bar chart from ops_gold.customer_spending)\n",
    "2. Revenue Trends: Monthly revenue progression over time (line chart from ops_gold.monthly_sales_summary)\n",
    "\n",
    "Execute both visualizations to complete the executive dashboard.\"\"\"\n",
    "\n",
    "bi_agent = llm.bind_tools(all_tools)\n",
    "dashboard_response = bi_agent.invoke(dashboard_prompt)\n",
    "\n",
    "if dashboard_response.tool_calls:\n",
    "    for tool_call in dashboard_response.tool_calls:\n",
    "        tool_function = {t.name: t for t in all_tools}[tool_call['name']]\n",
    "        result = tool_function.invoke(tool_call['args'])\n",
    "        log_event(\"INFO\", \"Dashboard\", f\"Visualization created: {tool_call['args'].get('title', 'Unknown')}\")\n",
    "else:\n",
    "    log_event(\"ERROR\", \"Dashboard\", \"Failed to generate dashboard visualizations\")\n",
    "\n",
    "log_event(\"INFO\", \"VALIDATION\", \"===== FINAL DATA VALIDATION =====\")\n",
    "\n",
    "validation_tables = [\n",
    "    \"ops_gold.customer_spending\",\n",
    "    \"ops_gold.monthly_sales_summary\",\n",
    "    \"Rejected.rejected_rows\" # Also validate the rejected table\n",
    "]\n",
    "\n",
    "for table in validation_tables:\n",
    "    try:\n",
    "        print(f\"\\n=== FINAL VALIDATION: {table} ===\")\n",
    "        df = spark.table(table)\n",
    "        print(f\"Record Count: {df.count()}\")\n",
    "        if \"customer_spending\" in table:\n",
    "            display(df.orderBy(col(\"total_spent\").desc()).limit(10))\n",
    "        elif \"monthly_sales\" in table:\n",
    "            display(df.orderBy(\"month\").limit(12))\n",
    "        else: # For rejected_rows\n",
    "             display(df.orderBy(col(\"rejection_timestamp\").desc()).limit(10))\n",
    "    except Exception as e:\n",
    "        log_event(\"ERROR\", \"Validation\", f\"Failed to validate {table}: {str(e)}\")\n",
    "\n",
    "log_event(\"INFO\", \"COMPLETION\", \"===== MEDALLION PIPELINE EXECUTION COMPLETE =====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 14: Final Success Message\n",
    "This final cell provides a concluding summary of the process. It prints a success message to confirm that the agentic workflows for all transformation tasks were initiated successfully. It reiterates that pull requests are ready for review and concisely summarizes the end-to-end workflow, from PR review to the final validation of the Gold and Rejected tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5de8877-09e3-42dd-9932-70e62f2998b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n🎉 SUCCESS: Databricks Medallion Architecture HITL pipeline initiated successfully!\")\n",
    "print(\"✅ All Bronze → Silver → Gold transformation workflows completed.\")\n",
    "print(\"✅ Pull Requests are waiting for your review in GitHub.\")\n",
    "print(\"➡️ Next Steps: Review PRs -> Merge -> CI/CD Execution -> Validate Gold & Rejected Tables.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "HITL ETL Agentic",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
